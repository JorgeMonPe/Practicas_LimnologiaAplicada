[["index.html", "Prácticas de Limnología Aplicada ¿Cómo seguir la práctica?", " Prácticas de Limnología Aplicada Jorge Juan Montes Pérez (jmontesp@uma.es) ¿Cómo seguir la práctica? Este documento está editado con bookdown. Lo podéis leer directamente en html (recomendado) o descargarlo tanto en formato pdf como en formato mobi. En la parte superior, encontraréis un icono para desplegar u ocultar la tabla de contenidos, un icono para buscar dentro del documento,un icono para descargar los formatos pdf y mobi y varios icono para compartir a través de distintas plataformas. A lo largo del documento entraréis “trocitos” de código R. Si pincháis en el recuadro code se desplegará todo el código y os aparecerá un icono para copiarlo. De esta manera, podréis ir copiando el código a vuestro script de R e ir siguiendo cada paso de la práctica. Prácticas Limnología Aplicada by Jorge Juan Montes Pérez is licensed under a Creative Commons Reconocimiento-NoComercial 4.0 Internacional License. "],["introducción.html", "1 Introducción 1.1 Metabolismo de ecosistemas acuáticos", " 1 Introducción 1.1 Metabolismo de ecosistemas acuáticos En el medio acuático tienen lugar todos los procesos metabólicos a través de los cuales la vida ha encontrado la manera de perpetuar su existencia modificando las sustancias que le rodean. La diversidad de las rutas a través de las cuales los organismos son capaces de obtener energía para realizar todos los procesos vitales es inabarcable en una práctica y, además, no es el objetivo final de esta. Sin embargo, las dos principales rutas que dominan el mundo aeróbico, debido a su gran eficiencia frente a las demás, son la fotosíntesis oxigénica y la respiración aeróbica 1, el resto de rutas encuentra su máxima relevancia en aquellos nichos en los que se dan las condiciones adecuadas y no son posible otros mecanismos más eficientes de obtención de energía. Por lo tanto, en un sistema acuático bien oxigenado los dos principales procesos que tienen un mayor impacto en el ecosistema son la fotosíntesis y la respiración. Del balance entre ambos procesos puede depender la cálidad del agua, tornándose anóxica, si prima la respiración por encima de los procesos de producción primaria, y comprometiendo muchas formas de vida. Además, ambos procesos juegan un papel relevante en el ciclo del carbono, retirando CO2 del entorno mediante la fotosíntesis (PP) o liberando CO2 a través de la respiración. Al igual que a nivel de organismo o de individuo, el metabolismo puede ser interpretado a nivel de ecosistema como el conjunto de procesos metabolicos que tienen lugar en dicho ecosistema. Como hemos dicho anteriormente, en un sistema acuático bien oxigenado, fotosíntesis y respiración son los procesos más importantes que determinan el metabolismo del ecosistema y su papel en los ciclos biogeoquímicos globales. Si queremos estimar el metabolismo aeróbico de un lago, los cambios en la concentración de oxígeno disuelto (OD) es una de las variables más utilizadas. Esta puede ser medida en botellas de incubación y monitorizar el cambio en la concentración de O2 durante un periodo de tiempo determinado y en un volumen de agua concreto. Esto ofrece la ventaja de controlar las condiciones ambientales (temperatura, irradiancia, etc) y poder realizar replicas. Sin embargo, el “efecto contenedor” conlleva ciertas limitaciones a la hora de escalar a nivel de ecosistema. Otra manera puede ser medir los cambios en la concentración de oxígeno directamente en la masa de agua (“aguas libres”), de este modo el escalado a nivel del conjunto de la masa es más realista, no sin ciertas límitaciones e incertidumbres Staehr et al. (2010). Sabemos que la respiración y la producción primaria afectan directamente a la concentración de OD, consumiendo oxígeno en el primer caso y produciendolo en el segundo. Sin embargo, hay otros factores físicos que pueden afectar a la concentración de OD (Ver figura). Por lo tanto, podríamos definir el cambio en la concentración de oxígeno con la siguiente ecuación: \\[\\frac{\\Delta OD}{\\Delta t} = GPP - R - At_{ex} - E\\] Donde, GPP es la producción primaria bruta, R es la respiración, Atex es el intercambio gaseoso con la atmósfera y E serían otros factores, de menor magnitud, que afectan a la concentración de OD como, por ejemplo, la oxidación fotoquímica de la materia orgánica o advección. Por lo tanto, si medimos los cambios que se producen en la concentración de OD durante un día completo en un lago, y podemos calcular el intercambio gaseoso de O2 que se produce entre la masa de agua y la atmósfera, podremos estimar su metabolismo. Por la noche, el cambio en la concentración de OD, no producto del intercambio gaseoso, será debido a la respiración. Durante el día, sin embargo, este cambio será debido a la producción neta (NEP). Conociendo que \\(NEP = GPP - R\\) y habiendo calculado la respiración, podemos averiguar cual es la producción primaria bruta del sistema. Este modelo tiene ciertas asunciones que conviene tener presente: Los cambios en la concentración de OD son el resultado del balance entre producción fotosintética y respiración así como del intercambio con la atmósfera. Es decir, asume que E es despreciable. La producción primaria tiene lugar solo durante el día. La tasa de respiración durante la noche es la misma que durante el día. Modelo conceptual de los principales factores físicos y biológicos que afecta a las variaciones en la concentración de oxígeno disuelto en un lago. Imagen extraída de Staehr et al. (2010). Referencias Staehr, P. a., D. Bade, G. R. Koch, C. Williamson, P. Hanson, J. J. Cole, and T. Kratz. 2010. Lake metabolism and the diel oxygen technique: State of the science. Limnology and Oceanography: Methods 8: 628–644. A partir de ahora, siempre que nos refiramos a la fotosíntesis o a la respiración a secas nos estamos refieriendo a la fotosíntesis oxigénica y la respiración aeróbica.↩︎ "],["estructura-de-la-práctica.html", "2 Estructura de la práctica", " 2 Estructura de la práctica Obtener los datos con los que vamos a trabajar. Usaremos la red The Global Lake Ecological Observatory Network (GLEON). Esta red pone a nuestra disposición una amplia cantidad de datos de monitoreo de alta frecuencia (HFM) de distintos lugares del mundo. Familiarizarnos con los paquetes y funciones básicos de R que nos facilitan explorar y trabajar con grandes tablas de datos. Estimar el metabolismo de un lago usando el paquete de R LakeMetabolizer. Explorar y representar los resultados obtenidos. "],["previsualización-de-los-datos.html", "3 Previsualización de los datos", " 3 Previsualización de los datos El primer paquete que vamos a usar es “ggplot2”. Si no lo tenéis instalado solo hay que ejecutar este comando en la consola: install.packages(\"ggplot2\"). Es un paquete muy útil y versátil para hacer gráficas. Aquí os dejo una “chuleta” donde se muestran las opciones de las que dispone el paquete: ggplot2 info. Si cerramos la sesión anterior y no tenemos el objeto en nuestro entorno debemos importar los datos que habíamos descargado. #Library library(ggplot2) #Cargamos el paquete #Cargo los datos que habíamos descargado de GLEON datos &lt;- read.csv(&quot;./Datos_descargados/Datos_Trout.csv&quot;) datos$sampledate &lt;- as.Date(datos$sampledate) #Creamos una variable que integra la fecha y la hora datetime &lt;- paste(datos$sampledate, datos$hour) datos$datetime &lt;- as.POSIXct(datetime, format = &quot;%Y-%m-%d %H&quot;) Vamos a echar una visual a los datos de OD, irradiancia, viento y temperatura del agua. Primero recordamos los nombres de las variables de nuestro data.frame: colnames(datos) ## [1] &quot;sampledate&quot; &quot;year4&quot; ## [3] &quot;month&quot; &quot;daynum&quot; ## [5] &quot;hour&quot; &quot;avg_air_temp&quot; ## [7] &quot;flag_avg_air_temp&quot; &quot;avg_rel_hum&quot; ## [9] &quot;flag_avg_rel_hum&quot; &quot;avg_wind_speed&quot; ## [11] &quot;flag_avg_wind_speed&quot; &quot;avg_wind_dir&quot; ## [13] &quot;flag_avg_wind_dir&quot; &quot;avg_do_raw&quot; ## [15] &quot;flag_avg_do_raw&quot; &quot;avg_do_sat&quot; ## [17] &quot;flag_avg_do_sat&quot; &quot;avg_do_wtemp&quot; ## [19] &quot;flag_avg_do_wtemp&quot; &quot;avg_barom_pres_mbar&quot; ## [21] &quot;flag_avg_barom_pres_mbar&quot; &quot;avg_par&quot; ## [23] &quot;flag_avg_par&quot; &quot;datetime&quot; Si tenéis alguna duda sobre cual es cada variable o sus unidades recordad que esa información la tenéis en los metadatos (View Full Metadata). #Previsualizamos valores de Oxigeno disuelto ggplot(datos, aes(x = datetime, y = avg_do_raw))+ #Aquí le indicamos que datos queremos representar y cuales son las variables x e y geom_point()+ #El tipo de gráfico que queremos, yo he elegido puntos facet_wrap(~year4, scales = &quot;free&quot;) #Le indicamos que haga una gráfica por año ## Warning: Removed 7550 rows containing missing values (geom_point). Lo primero que nos dice ggplot es: ## Warning: Removed 7550 rows containing missing values (geom_point), empezamos bien… Bueno no os preocupéis en realidad ggplot solo nos está avisando de que tenemos 7550 filas en las que no hay ningún valor para la variable que hemos representado (OD). Esto es normal, en una base de datos tan grande es habitual que no todas las variables estén disponibles para todo el periodo (el sensor de oxígeno se estropea, tienen que quitar la boya por cuestiones técnicas, etc). Si echamos un vistazo a las gráficas podemos notar varias cosas. Por un lado, parece que no tenemos datos para el invierno, los datos van, generalmente, desde principios de junio hasta finales de noviembre. Por otro lado, cabe resaltar que los valores de OD oscilan entre 8-14 mg/L excepto en los años 2017 que tenemos valores negativos y algunos muy altos de 40 mg/L. Si quisieramos indagar un poco más: length(which(datos$avg_do_raw &lt; 0 | datos$avg_do_raw &gt; 20)) #Vemos cuantos datos se salen del rango de concentración esperado. ## [1] 43 #También podemos seleccionar los datos extraños y ver si alguna variable más muestra valores incoherentes datos_OD_out &lt;- subset(datos, avg_do_raw &lt; 0 | avg_do_raw &gt; 20) summary(datos_OD_out) ## sampledate year4 month daynum ## Min. :2016-08-14 Min. :2016 Min. : 7.000 Min. :189.0 ## 1st Qu.:2016-08-15 1st Qu.:2016 1st Qu.: 8.000 1st Qu.:228.0 ## Median :2016-08-16 Median :2016 Median : 8.000 Median :228.0 ## Mean :2016-11-23 Mean :2016 Mean : 7.953 Mean :226.7 ## 3rd Qu.:2016-08-16 3rd Qu.:2016 3rd Qu.: 8.000 3rd Qu.:229.0 ## Max. :2018-10-15 Max. :2018 Max. :10.000 Max. :288.0 ## hour avg_air_temp flag_avg_air_temp avg_rel_hum ## Min. : 0.00 Min. : 1.31 Length:43 Min. :45.27 ## 1st Qu.: 7.50 1st Qu.:19.30 Class :character 1st Qu.:55.00 ## Median :12.00 Median :21.63 Mode :character Median :67.43 ## Mean :11.28 Mean :21.05 Mean :68.07 ## 3rd Qu.:14.50 3rd Qu.:23.89 3rd Qu.:81.36 ## Max. :23.00 Max. :25.89 Max. :88.41 ## flag_avg_rel_hum avg_wind_speed flag_avg_wind_speed avg_wind_dir ## Length:43 Min. :0.760 Length:43 Min. : 81.43 ## Class :character 1st Qu.:1.375 Class :character 1st Qu.:153.74 ## Mode :character Median :1.840 Mode :character Median :200.07 ## Mean :2.406 Mean :198.45 ## 3rd Qu.:2.855 3rd Qu.:233.00 ## Max. :8.460 Max. :305.03 ## flag_avg_wind_dir avg_do_raw flag_avg_do_raw avg_do_sat ## Length:43 Min. :-7999.00 Length:43 Min. :-7999.00 ## Class :character 1st Qu.:-7999.00 Class :character 1st Qu.:-7999.00 ## Mode :character Median :-7999.00 Mode :character Median :-7999.00 ## Mean :-5928.70 Mean :-5930.95 ## 3rd Qu.:-2642.49 3rd Qu.:-2583.11 ## Max. : 42.12 Max. : 98.83 ## flag_avg_do_sat avg_do_wtemp flag_avg_do_wtemp avg_barom_pres_mbar ## Length:43 Min. :-7999.00 Length:43 Min. :958.0 ## Class :character 1st Qu.:-7999.00 Class :character 1st Qu.:960.0 ## Mode :character Median :-7999.00 Mode :character Median :960.0 ## Mean :-5917.54 Mean :960.9 ## 3rd Qu.:-2633.04 3rd Qu.:961.0 ## Max. : 59.23 Max. :969.0 ## flag_avg_barom_pres_mbar avg_par flag_avg_par ## Mode:logical Min. : 0.00 Length:43 ## NA&#39;s:43 1st Qu.: 35.87 Class :character ## Median :347.87 Mode :character ## Mean :341.32 ## 3rd Qu.:603.67 ## Max. :842.21 ## datetime ## Min. :2016-08-14 14:00:00.00 ## 1st Qu.:2016-08-15 17:30:00.00 ## Median :2016-08-16 04:00:00.00 ## Mean :2016-11-24 03:01:23.72 ## 3rd Qu.:2016-08-16 14:30:00.00 ## Max. :2018-10-15 12:00:00.00 #¿A que días afecta? unique(datos_OD_out$sampledate) ## [1] &quot;2016-08-14&quot; &quot;2016-08-15&quot; &quot;2016-08-16&quot; &quot;2016-08-17&quot; &quot;2017-08-05&quot; ## [6] &quot;2017-08-25&quot; &quot;2018-07-08&quot; &quot;2018-07-11&quot; &quot;2018-07-16&quot; &quot;2018-07-18&quot; ## [11] &quot;2018-10-15&quot; Parece que las únicas variables que también se ven afectadas son oxígeno en saturación y temperatura del agua. Posiblemente esas variables se registran con el mismo sensor y hubo algún fallo en él. "],["descargar-los-datos-de-la-red-gleon.html", "4 Descargar los datos de la red GLEON", " 4 Descargar los datos de la red GLEON Para ello, visitamos la página de la red GLEON y nos vamos al apartado de datos. En esta sección podemos encontrar la política de datos de GLEON, basicamente se apuesta por una ciencia colaborativa en la que los datos quedan a disposición de la comunidad para cualquier fin de investigación, académico, educativo o cualquier otro, siempre que no haya un interés lucrativo detrás y respetando algunos principios de comunicación con los responsables de los datos. Como se muestra en esta sección, a los datos de GLEON se puede acceder a través de tres buscadores EDI, DataONE o Google data set. Pues bien, para este práctica vamos a trabajar, en concreto, con datos del lago Trout. Así que utilizando el buscador que más sea de vuestro agrado lanzamos la siguiente busqueda: ecosystem metabolism lake trout. Entre los resultados obtenidos (hay bastante información como podéis observar), vamos a seleccionar los datos derivados del proyecto North Temperate Lakes Long Term Ecologycal Research (NTL-LTER) que nos ofrecen datos meteorológicos y de oxígeno disuelto desde 2004 hasta la actualidad. Si no pudierais encontrarlos, podéis pinchar aquí: North Temperate Lakes LTER: High Frequency Meteorological and Dissolved Oxygen Data - Trout Lake Buoy 2004 - current. En esa página que acabais de abrir tenéis un sumario con toda la información necesaria sobre el paquete de datos (Title, Creators, Publication Date, Citation, Abstract, Spatial Coverage, Package ID, Resources, Intellectual Rights, Digital Object Identifier, PASTA Identifier, Code Generation, Provenance, Journal Citations). Las que más nos van a interesar por el momento son Abstract, Resources y Code Generation. La primera de ellas es un resumen que nos explica como han sido recogido los datos y algunas particularidades que debemos saber, en la segunda tenemos directamente los archivos con los datos para descargarlos en formato *.csv y una opción muy interesante, View Full Metadata, en la que si desplegamos Data Entities podemos ver información sobre las variables que aparecen en la tabla de datos como, por ejemplo, las unidades en las que están medidas. Disponemos de dos resoluciones temporales de datos, una diaria y otra horaria. Si queremos estimar el metabolismo no nos sirven los datos diarios. Para descargar los datos tenemos dos opciones: Podemos pinchar directamente en el archivo y descargarlo a través del navegador. Name: North Temperate Lakes LTER - Hourly Meteorological and Dissolved Oxygen Data - Trout Lake File: ntl117_2_v4.csv (4.4M; 13 downloads). Si optamos por esta opción, posteriormente habrá que importar los datos a R. Otra opción mucho más cómoda es la de usar un script de R que ya nos han preparado para facilitarnos la descarga e importación. Para ello, pinchamos en el icono de R el apartado Code Generation. Como véis también hay opción para descargar y trabajar directamente los datos con otras herramientas, si a alguno le pica la curiosidad ¡adelante!. En esta práctica como hemos dicho vamos a usar R, debido a que es una herramienta gratuita, de código abierto y libre. Además es ampliamente usado en investigación debido a su naturaleza libre y colaborativa. En fin, si más dilaciones, podéis decargar el script pinchando en File Download: knb-lter-ntl.117.38.r y abrirlo con RStudio y ejecutarlo (solo el trozo que nos interesa). Esta es la pinta que tiene el script: # Package ID: knb-lter-ntl.117.38 Cataloging System:https://pasta.edirepository.org. # Data set title: North Temperate Lakes LTER: High Frequency Meteorological and Dissolved Oxygen Data - Trout Lake Buoy 2004 - current. # Data set creator: NTL Lead PI - University of Wisconsin # Data set creator: John Magnuson - University of Wisconsin # Data set creator: Stephen Carpenter - University of Wisconsin # Data set creator: Emily Stanley - University of Wisconsin # Metadata Provider: NTL Information Manager - University of Wisconsin # Contact: NTL Information Manager - University of Wisconsin - ntl.infomgr@gmail.com # Contact: NTL Lead PI - University of Wisconsin - ntl.leadpi@gmail.com # Stylesheet v2.7 for metadata conversion into program: John H. Porter, Univ. Virginia, jporter@virginia.edu inUrl1 &lt;- &quot;https://pasta.lternet.edu/package/data/eml/knb-lter-ntl/117/38/e7cef73c65218cbf1acd1ff460969029&quot; infile1 &lt;- tempfile() download.file(inUrl1,infile1,method=&quot;curl&quot;) dt1 &lt;-read.csv(infile1,header=F ,skip=1 ,sep=&quot;,&quot; ,quot=&#39;&quot;&#39; , col.names=c( &quot;sampledate&quot;, &quot;year4&quot;, &quot;month&quot;, &quot;daynum&quot;, &quot;avg_air_temp&quot;, &quot;flag_avg_air_temp&quot;, &quot;avg_rel_hum&quot;, &quot;flag_avg_rel_hum&quot;, &quot;avg_wind_speed&quot;, &quot;flag_avg_wind_speed&quot;, &quot;avg_wind_dir&quot;, &quot;flag_avg_wind_dir&quot;, &quot;avg_do_raw&quot;, &quot;flag_avg_do_raw&quot;, &quot;avg_do_sat&quot;, &quot;flag_avg_do_sat&quot;, &quot;avg_do_wtemp&quot;, &quot;flag_avg_do_wtemp&quot;, &quot;avg_barom_pres_mbar&quot;, &quot;flag_avg_barom_pres_mbar&quot;, &quot;avg_par&quot;, &quot;flag_avg_par&quot; ), check.names=TRUE) Esta primera parte del script es para descargar los datos diarios, los cuales dijimos que no nos interesaban, por lo tanto vamos más abajo en el script (entre las líneas 110-207) para descargar los datos por hora. Este es aspecto que tiene (salvo que me he tomado el tiempo de comentar algunas líneas): inUrl2 &lt;- &quot;https://pasta.lternet.edu/package/data/eml/knb-lter-ntl/117/38/7f36b24e62c5798b16517e9dd85fd628&quot; #Esta es la dirección de donde descarga los datos infile2 &lt;- tempfile() #Crea un objeto temporal donde posteriormente guardar los datos download.file(inUrl2,infile2,method=&quot;curl&quot;) #Decarga los datos al objeto temporal (A vosotros os debe aparecer method = &quot;auto&quot;, aquí esta modificado el método para poder descargarlo dentro del documento.) dt2 &lt;-read.csv(infile2,header=F #Crea un objeto donde &quot;vuelca los datos descargados&quot; ,skip=1 ,sep=&quot;,&quot; ,quot=&#39;&quot;&#39; , col.names=c( &quot;sampledate&quot;, #Asigna nombre a las columnas &quot;year4&quot;, &quot;month&quot;, &quot;daynum&quot;, &quot;hour&quot;, &quot;avg_air_temp&quot;, &quot;flag_avg_air_temp&quot;, &quot;avg_rel_hum&quot;, &quot;flag_avg_rel_hum&quot;, &quot;avg_wind_speed&quot;, &quot;flag_avg_wind_speed&quot;, &quot;avg_wind_dir&quot;, &quot;flag_avg_wind_dir&quot;, &quot;avg_do_raw&quot;, &quot;flag_avg_do_raw&quot;, &quot;avg_do_sat&quot;, &quot;flag_avg_do_sat&quot;, &quot;avg_do_wtemp&quot;, &quot;flag_avg_do_wtemp&quot;, &quot;avg_barom_pres_mbar&quot;, &quot;flag_avg_barom_pres_mbar&quot;, &quot;avg_par&quot;, &quot;flag_avg_par&quot; ), check.names=TRUE) Este trocito de código que sigue es para corregir algún problema de formato que se haya podido introducir debido a algún error en la base de datos. # Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings # attempting to convert dt2$sampledate dateTime string to R date structure (date or POSIXct) tmpDateFormat&lt;-&quot;%Y-%m-%d&quot; tmp2sampledate&lt;-as.Date(dt2$sampledate,format=tmpDateFormat) # Keep the new dates only if they all converted correctly if(length(tmp2sampledate) == length(tmp2sampledate[!is.na(tmp2sampledate)])){dt2$sampledate &lt;- tmp2sampledate } else {print(&quot;Date conversion failed for dt2$sampledate. Please inspect the data and do the date conversion yourself.&quot;)} rm(tmpDateFormat,tmp2sampledate) if (class(dt2$year4)==&quot;factor&quot;) dt2$year4 &lt;-as.numeric(levels(dt2$year4))[as.integer(dt2$year4) ] if (class(dt2$month)==&quot;factor&quot;) dt2$month &lt;-as.numeric(levels(dt2$month))[as.integer(dt2$month) ] if (class(dt2$daynum)==&quot;factor&quot;) dt2$daynum &lt;-as.numeric(levels(dt2$daynum))[as.integer(dt2$daynum) ] if (class(dt2$avg_air_temp)==&quot;factor&quot;) dt2$avg_air_temp &lt;-as.numeric(levels(dt2$avg_air_temp))[as.integer(dt2$avg_air_temp) ] if (class(dt2$flag_avg_air_temp)!=&quot;factor&quot;) dt2$flag_avg_air_temp&lt;- as.factor(dt2$flag_avg_air_temp) if (class(dt2$avg_rel_hum)==&quot;factor&quot;) dt2$avg_rel_hum &lt;-as.numeric(levels(dt2$avg_rel_hum))[as.integer(dt2$avg_rel_hum) ] if (class(dt2$flag_avg_rel_hum)!=&quot;factor&quot;) dt2$flag_avg_rel_hum&lt;- as.factor(dt2$flag_avg_rel_hum) if (class(dt2$avg_wind_speed)==&quot;factor&quot;) dt2$avg_wind_speed &lt;-as.numeric(levels(dt2$avg_wind_speed))[as.integer(dt2$avg_wind_speed) ] if (class(dt2$flag_avg_wind_speed)!=&quot;factor&quot;) dt2$flag_avg_wind_speed&lt;- as.factor(dt2$flag_avg_wind_speed) if (class(dt2$avg_wind_dir)==&quot;factor&quot;) dt2$avg_wind_dir &lt;-as.numeric(levels(dt2$avg_wind_dir))[as.integer(dt2$avg_wind_dir) ] if (class(dt2$flag_avg_wind_dir)!=&quot;factor&quot;) dt2$flag_avg_wind_dir&lt;- as.factor(dt2$flag_avg_wind_dir) if (class(dt2$avg_do_raw)==&quot;factor&quot;) dt2$avg_do_raw &lt;-as.numeric(levels(dt2$avg_do_raw))[as.integer(dt2$avg_do_raw) ] if (class(dt2$flag_avg_do_raw)!=&quot;factor&quot;) dt2$flag_avg_do_raw&lt;- as.factor(dt2$flag_avg_do_raw) if (class(dt2$avg_do_sat)==&quot;factor&quot;) dt2$avg_do_sat &lt;-as.numeric(levels(dt2$avg_do_sat))[as.integer(dt2$avg_do_sat) ] if (class(dt2$flag_avg_do_sat)!=&quot;factor&quot;) dt2$flag_avg_do_sat&lt;- as.factor(dt2$flag_avg_do_sat) if (class(dt2$avg_do_wtemp)==&quot;factor&quot;) dt2$avg_do_wtemp &lt;-as.numeric(levels(dt2$avg_do_wtemp))[as.integer(dt2$avg_do_wtemp) ] if (class(dt2$flag_avg_do_wtemp)!=&quot;factor&quot;) dt2$flag_avg_do_wtemp&lt;- as.factor(dt2$flag_avg_do_wtemp) if (class(dt2$avg_barom_pres_mbar)==&quot;factor&quot;) dt2$avg_barom_pres_mbar &lt;-as.numeric(levels(dt2$avg_barom_pres_mbar))[as.integer(dt2$avg_barom_pres_mbar) ] if (class(dt2$flag_avg_barom_pres_mbar)!=&quot;factor&quot;) dt2$flag_avg_barom_pres_mbar&lt;- as.factor(dt2$flag_avg_barom_pres_mbar) if (class(dt2$avg_par)==&quot;factor&quot;) dt2$avg_par &lt;-as.numeric(levels(dt2$avg_par))[as.integer(dt2$avg_par) ] if (class(dt2$flag_avg_par)!=&quot;factor&quot;) dt2$flag_avg_par&lt;- as.factor(dt2$flag_avg_par) # Convert Missing Values to NA for non-dates Este último trozo es simplemente para ver la estructura de los datos y un resumen de cada una de la variables. Este trozo no es necesario que lo ejecutéis. # Here is the structure of the input data frame: str(dt2) ## &#39;data.frame&#39;: 55255 obs. of 23 variables: ## $ sampledate : Date, format: &quot;2004-05-21&quot; &quot;2004-05-21&quot; ... ## $ year4 : int 2004 2004 2004 2004 2004 2004 2004 2004 2004 2004 ... ## $ month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ daynum : int 142 142 142 142 142 142 142 142 142 143 ... ## $ hour : int 15 16 17 18 19 20 21 22 23 0 ... ## $ avg_air_temp : num 0 0 0 0 0 0 0 0 0 0 ... ## $ flag_avg_air_temp : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ avg_rel_hum : num NA NA NA NA NA NA NA NA NA NA ... ## $ flag_avg_rel_hum : Factor w/ 3 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ avg_wind_speed : num NA NA NA NA NA NA NA NA NA NA ... ## $ flag_avg_wind_speed : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ avg_wind_dir : num 226 246 256 295 290 ... ## $ flag_avg_wind_dir : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ avg_do_raw : num 11.1 11.1 11.1 11.1 11.1 ... ## $ flag_avg_do_raw : Factor w/ 5 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ avg_do_sat : num 99.5 99.5 99.5 99.5 99.4 ... ## $ flag_avg_do_sat : Factor w/ 5 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ avg_do_wtemp : num 10.5 10.5 10.5 10.5 10.4 ... ## $ flag_avg_do_wtemp : Factor w/ 5 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;H&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ avg_barom_pres_mbar : num NA NA NA NA NA NA NA NA NA NA ... ## $ flag_avg_barom_pres_mbar: Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ... ## $ avg_par : num 207.8 76.2 69 67.7 5.7 ... ## $ flag_avg_par : Factor w/ 5 levels &quot;&quot;,&quot;C&quot;,&quot;D&quot;,&quot;F&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... attach(dt2) # The analyses below are basic descriptions of the variables. After testing, they should be replaced. summary(sampledate) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## &quot;2004-05-21&quot; &quot;2006-10-02&quot; &quot;2010-08-22&quot; &quot;2010-12-09&quot; &quot;2014-07-15&quot; &quot;2018-11-12&quot; summary(year4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2004 2006 2010 2010 2014 2018 summary(month) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 7.00 8.00 7.98 9.00 11.00 summary(daynum) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 31.0 190.0 233.0 227.5 272.0 334.0 summary(hour) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 5.5 11.0 11.5 18.0 23.0 summary(avg_air_temp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -12.24 6.72 15.11 13.26 19.77 31.68 summary(flag_avg_air_temp) ## C D H ## 48513 6727 14 1 summary(avg_rel_hum) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 26.79 64.60 77.36 74.74 86.68 103.60 6928 summary(flag_avg_rel_hum) ## C D ## 48487 6752 16 summary(avg_wind_speed) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 2.190 3.600 3.904 5.100 1000.000 10785 summary(flag_avg_wind_speed) ## C D H ## 47779 7238 233 5 summary(avg_wind_dir) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 148.6 211.7 210.0 284.9 1000.0 7324 summary(flag_avg_wind_dir) ## C D H ## 47991 6861 398 5 summary(avg_do_raw) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -7999.000 8.690 9.250 4.306 10.330 42.120 7550 summary(flag_avg_do_raw) ## C D H O ## 47888 3771 1700 1887 9 summary(avg_do_sat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -7999.00 92.89 99.93 94.99 104.65 133.97 7552 summary(flag_avg_do_sat) ## C D H O ## 48678 3758 1718 1092 9 summary(avg_do_wtemp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -7999.00 14.05 19.12 12.21 21.64 59.99 7705 summary(flag_avg_do_wtemp) ## C D H O ## 51559 2503 1087 97 9 summary(avg_barom_pres_mbar) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 931.0 956.2 960.0 959.9 964.0 977.0 44268 summary(flag_avg_barom_pres_mbar) ## NA&#39;s ## 55255 summary(avg_par) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -0.67 0.00 47.39 295.80 471.45 2024.52 19557 summary(flag_avg_par) ## C D F H ## 43248 5235 29 6722 21 detach(dt2) Bien, una vez ejecutado el script, ya debemos tener los datos en nuestro entorno de RStudio en un tipo de objeto denominado data.frame (básicamente una tabla de datos). Vamos a ver que pinta tienen: head(dt2) ## sampledate year4 month daynum hour avg_air_temp flag_avg_air_temp avg_rel_hum ## 1 2004-05-21 2004 5 142 15 0 C NA ## 2 2004-05-21 2004 5 142 16 0 C NA ## 3 2004-05-21 2004 5 142 17 0 C NA ## 4 2004-05-21 2004 5 142 18 0 C NA ## 5 2004-05-21 2004 5 142 19 0 C NA ## 6 2004-05-21 2004 5 142 20 0 C NA ## flag_avg_rel_hum avg_wind_speed flag_avg_wind_speed avg_wind_dir ## 1 C NA C 226.25 ## 2 C NA C 246.27 ## 3 C NA C 255.92 ## 4 C NA C 294.78 ## 5 C NA C 289.70 ## 6 C NA C 288.42 ## flag_avg_wind_dir avg_do_raw flag_avg_do_raw avg_do_sat flag_avg_do_sat ## 1 11.09 99.50 ## 2 11.10 99.50 ## 3 11.11 99.50 ## 4 11.11 99.50 ## 5 11.11 99.42 ## 6 11.09 99.18 ## avg_do_wtemp flag_avg_do_wtemp avg_barom_pres_mbar flag_avg_barom_pres_mbar ## 1 10.53 NA &lt;NA&gt; ## 2 10.52 NA &lt;NA&gt; ## 3 10.49 NA &lt;NA&gt; ## 4 10.48 NA &lt;NA&gt; ## 5 10.45 NA &lt;NA&gt; ## 6 10.43 NA &lt;NA&gt; ## avg_par flag_avg_par ## 1 207.85 F ## 2 76.17 F ## 3 68.95 F ## 4 67.72 F ## 5 5.70 F ## 6 0.00 F Por último, los vamos a guardar en la carpeta “Datos_descargados” que hemos creado.. write.csv(dt2, &quot;./Datos_descargados/Datos_Trout.csv&quot;, row.names = FALSE) "],["estima-del-metabolismo-del-metabolismo-aeróbico..html", "5 Estima del metabolismo del metabolismo aeróbico. 5.1 Función metab() del paquete LakeMetabolizer 5.2 Volvemos a la función metab()", " 5 Estima del metabolismo del metabolismo aeróbico. Una vez que hemos visualizado las variables de las que disponemos. Vamos a buscar un periodo que nos interese. En este caso nos vamos a fijar solamente en que estén los datos que necesitamos. Sin embargo, con el resto de variables podríamos explorar los datos y buscar un periodo de interés concreto en base a un evento particular (un verano muy caluroso, una época muy ventosa o un periodo de bajas presiones). Primero vamos a cargar los datos de la sesión anterior. #Importo los datos datos &lt;- read.csv(&quot;./Datos_descargados/Datos_Trout.csv&quot;) datos$sampledate &lt;- as.Date(datos$sampledate) #Creamos una variable que integra la fecha y la hora datetime &lt;- paste(datos$sampledate, datos$hour) datos$datetime &lt;- as.POSIXct(datetime, format = &quot;%Y-%m-%d %H&quot;) Seleccionamos un subgrupo de datos en los que tengamos información de OD, temperatura del agua, velocidad del viento y radiación fotosintéticamente activa (PAR). Limpios &lt;- subset(datos, !is.na(avg_do_raw) &amp; !is.na(avg_wind_speed) &amp; !is.na(avg_do_wtemp) &amp; !is.na(avg_par)) Nosotros, de aquí, vamos a seleccionar un par de meses de 2005. Desde &lt;- &quot;2005-09-01&quot; Hasta &lt;- &quot;2005-10-31&quot; #Ahora seleccionamos el periodo en nuestro datos Periodo_sel &lt;- subset(Limpios, sampledate &gt;= Desde &amp; sampledate &lt;= Hasta) 5.1 Función metab() del paquete LakeMetabolizer En primer lugar deberéis instalar el paquete y cargalo: install.packages(&quot;LakeMetabolizer&quot;) library(LakeMetabolizer) Aquí tenéis información sobre el paquete: 1.Paquete info LakeMetabolizer. 2.Artículo. La función que vamos a usar es metab(). Si queremos saber como usarla, lo que debemos hacer es preguntarle a R ?metab(). En el apartado “Help” de Rstudio nos aparece la información necesaria sobre la función: Para qué se usa, cómo se usa, los argumentos que necesita y los valores que te devuelve. Si echamos un vistazo a los argumentos vemos que la información que necesitamos es la siguiente: Oxígeno disuelto en mg/L. Para obtener un buena estima del metabolismo lo ideal sería de disponer de una medida de oxígeno al menos cada 30 minutos (Staehr et al. 2010). Concentración de oxígeno disuelto (mg/L) que tendría el agua si estuviera saturada al 100%. Coeficiente de intercambio gaseoso en m/día. Profundidad de la capa de mezcla en m. Irradiancia. Temperatura del agua. Si revisamos la información de la que disponemos vemos que algunas de ellas ya las tenemos: oxígeno disuelto (avg_do_raw), irradiancia (avg_par) y temperatura del agua (avg_do_wtemp). Sin embargo, no disponemos de todas. Por lo tanto, vamos a intentar obtenerlas todas y, más tarde, cuando las tengamos todas, volveremos de nuevo a la función metab(). 5.1.1 Concentración de oxígeno en saturación. Para calcular la concentración de oxígeno en saturación, el mismo paquete LakeMetabolizer nos provee una función denominada o2.at.sat(). Lo primero que hacemos es ver como funciona ?o2.at.sat(). Vemos que necesitamos un data.frame con dos columnas, una con la fecha y la hora (datetime) y otra con la temperatura del agua en \\(^oC\\) (wtr). Además, tenemos la opción de introducir la presión barométrica en milibares o la altitud en metros a la que se encuentra el lago. Aunque en nuestros datos tenemos una variable que corresponde con la presión atmosférica parece, por desgracia, que no disponemos de datos para el periodo seleccionado. Bueno, en su lugar podemos usar la altitud a la que se encuentra el lago que sabemos que es de 492 m (Info Trout Lake). La salinidad está por defecto en 0, así que como nuestro lago es de agua dulce no debemos cambiarla. saturacion &lt;- Periodo_sel[,c(&quot;datetime&quot;,&quot;avg_do_wtemp&quot;)] #Creamos un data.frame con las dos variables que necesitamos colnames(saturacion) &lt;- c(&quot;datetime&quot;, &quot;wtr&quot;) #Renombramos las columnas para que la función las reconozca automaticamente. results_sat &lt;- o2.at.sat(saturacion, altitude = 492) Ya tenemos la concentración de oxígeno en saturación calculada, vamos a por la siguiente variable. 5.1.2 Coeficiente de intercambio gaseoso Para esto, el paquete LakeMetabolizer también dispone de un función que lo calcula k600.2.kGAS(). Pero para aplicar esta función antes debemos calcular k600. 5.1.2.1 Calcular k600 Si ejecutamos en la consola ?k.read() podremos ver la distintas opciones que nos ofrece el paquete para calcular la k600. Nosotros vamos a usar el método que proponen Cole and Caraco (1998), para ello usamos la función k.cole(). Aquí, la información de la función está, cuanto menos, confusa. Para aplicar la función necesitamos un data.frame con dos columnas, una con la fecha y la hora (datetime) y otra con la velocidad del viento en m/s a 10 metros de altura (wnd). El sensor de “nuestra” boya registra la velocidad del viento a 2 m de altura (Dugan et al. 2016), por lo tanto debemos calcular la velocidad del viento a 10 m. Para ello tambien tenemos una función disponible wind.scale(). df_k &lt;- Periodo_sel[,c(&quot;datetime&quot;,&quot;avg_wind_speed&quot;)] #Creamos un objeto solo con las variables que nos interesan. colnames(df_k)[2] &lt;- &quot;wnd&quot; #renombramos la columna con los datos de viento #Calculamos la velocidad del viento a 10 m df_k &lt;- wind.scale(df_k, wnd.z = 2) head(df_k) ## datetime wnd_10 ## 1 2005-09-01 00:00:00 5.410463 ## 2 2005-09-01 01:00:00 6.645322 ## 3 2005-09-01 02:00:00 5.028548 ## 4 2005-09-01 03:00:00 3.997377 ## 5 2005-09-01 04:00:00 3.793689 ## 6 2005-09-01 05:00:00 4.302909 Ahora sí podemos calcular la k600. k_600 &lt;- k.cole(df_k) head(k_600) ## datetime k600 ## 1 2005-09-01 00:00:00 1.4070239 ## 2 2005-09-01 01:00:00 1.7878020 ## 3 2005-09-01 02:00:00 1.3005151 ## 4 2005-09-01 03:00:00 1.0408859 ## 5 2005-09-01 04:00:00 0.9945998 ## 6 2005-09-01 05:00:00 1.1134595 Una vez hemos calculado la k600, y teniendo la temperatura del agua, podemos calcular el coeficiente de intercambio gaseoso para el O2. #Añadimos la variable temperatura k_600$wtr &lt;- Periodo_sel$avg_do_wtemp k_gas &lt;- k600.2.kGAS(k_600, gas = &quot;O2&quot;) Ahora juntamos todos los datos y los ordenamos. Aprovecho para presentaros el paquete dplyr. Aquí tenéis una visión general del paquete. install.packages(&quot;dplyr&quot;) library(&quot;dplyr&quot;) #juntamos todos los datos datos.ts &lt;- merge(Periodo_sel, results_sat) %&gt;% merge(k_gas) #seleccionamos los que nos atañen. datos.ts &lt;- datos.ts[,c(&quot;datetime&quot;, &quot;avg_do_raw&quot;, &quot;do.sat&quot;, &quot;k.gas&quot;, &quot;avg_par&quot;,&quot;avg_do_wtemp&quot;)] #Renombramos las columnas de oxigeno, irradiancia y temperatura para que las interprete la función metab() colnames(datos.ts)[c(2,5,6)] &lt;- c(&quot;do.obs&quot;, &quot;irr&quot;, &quot;wtr&quot;) Además debemos añadir la profundidad de la termoclina. Nosotros no disponemos del perfil del temperatura (aún) por lo que no podemos saber donde se sitúa la termoclina, por lo tanto no nos queda más que asumir que el lago permanece mezclado durante todo el periodo. datos.ts$z.mix &lt;- 15 5.2 Volvemos a la función metab() Bien, ya tenemos los datos que necesitamos ordenaditos en un data.frame, ahora podemos calcular el metabolismo. Para ello, la función nos ofrece 5 métodos para estimar el metabolismo. El más sencillo y el que menos asunciones toma, es el método ‘bookkeep’ Odum (1956), por lo tanto será el que usemos: Metabolismo &lt;- metab(datos.ts, method = &quot;bookkeep&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 48&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; ## Error in metab.bookkeep(do.obs = c(8.07, 8.05, 8.02, 8.01, 8.06, 8.11, : either supply datetime &amp; lake.lat arguments, or supply irr as integer vector of 1&#39;s and 0&#39;s El modelo nos pide que le introduzcamos también la latitud a la que se encuentra el lago o que el vector irradiancia sea de 0 y 1 (noche o día), esto es porque el método “bookkeep” no usa ninguna relación entre GPP e irradiancia, por lo tanto sólo le sirve para saber cuando se es de día o de noche. Metabolismo_Trout &lt;- metab(datos.ts, lake.lat = 46, method = &quot;bookkeep&quot;) ## [1] &quot;Points removed due to incomplete day or duplicated time step: 48&quot; ## [1] &quot;NA&#39;s added to fill in time series: 0&quot; No os preocupéis por este mensaje. Nos informa que cuando no tiene la información completa para un día no puede calcular el metabolismo para ese día, así que se lo salta y sigue con el siguiente. head(Metabolismo_Trout) ## year doy GPP R NEP ## 1 2005 244 0.16177941 -0.10095561 0.01957457 ## 2 2005 245 0.25551422 -0.29238301 -0.16559482 ## 3 2005 246 0.07526482 0.01752214 0.10443949 ## 4 2005 247 0.12384506 -0.11163144 -0.03579075 ## 5 2005 248 0.27890580 -0.21474973 -0.02642397 ## 6 2005 249 0.23568249 -0.26489782 -0.14565852 summary(Metabolismo_Trout) ## year doy GPP R ## Min. :2005 Min. :244.0 Min. :-0.8299 Min. :-1.6411 ## 1st Qu.:2005 1st Qu.:258.5 1st Qu.: 0.1738 1st Qu.:-0.3120 ## Median :2005 Median :273.0 Median : 0.2640 Median :-0.2143 ## Mean :2005 Mean :273.0 Mean : 0.2945 Mean :-0.2311 ## 3rd Qu.:2005 3rd Qu.:287.5 3rd Qu.: 0.3763 3rd Qu.:-0.1210 ## Max. :2005 Max. :302.0 Max. : 1.8408 Max. : 0.5316 ## NEP ## Min. :-0.55284 ## 1st Qu.:-0.13305 ## Median :-0.05037 ## Mean :-0.04596 ## 3rd Qu.: 0.01869 ## Max. : 0.61800 Ahora sí, ya hemos calculado el metabolismo (GPP, R y NEP) en el lago Trout durante el periodo de junio-octubre 2018. Con el paquete ggplot2 podemos comenzar a explorar los datos. También podemos guardar los resultados en la carpeta “Datos” para trabajar con ellos en otro momento. write.csv(datos.ts, &quot;./Datos/datos_ts_2005.csv&quot;, row.names = FALSE) write.csv(Metabolismo_Trout, &quot;./Datos/Metabolismo_Trout.csv&quot;, row.names = FALSE) Referencias Cole, J. J., and N. F. Caraco. 1998. Atmospheric exchange of carbon dioxide in a low-wind oligotrophic lake measured by the addition of SF 6. Limnology and Oceanography 43: 647–656. Dugan, H. A., R. Iestyn Woolway, A. B. Santoso, J. R. Corman, A. Jaimes, E. R. Nodine, V. P. Patil, J. A. Zwart, J. A. Brentrup, A. L. Hetherington, S. K. Oliver, J. S. Read, K. M. Winters, P. C. Hanson, E. K. Read, L. A. Winslow, and K. C. Weathers. 2016. Consequences of gas flux model choice on the interpretation of metabolic balance across 15 lakes. Inland Waters 6: 581–592. Odum, H. T. 1956. Primary Production in Flowing Waters. Limnology and Oceanography 102–117. Staehr, P. a., D. Bade, G. R. Koch, C. Williamson, P. Hanson, J. J. Cole, and T. Kratz. 2010. Lake metabolism and the diel oxygen technique: State of the science. Limnology and Oceanography: Methods 8: 628–644. "],["estabilidad-térmica-de-la-columna-de-agua..html", "6 Estabilidad térmica de la columna de agua.", " 6 Estabilidad térmica de la columna de agua. Como bien sabéis, aunque hayamos asumido que la profundidad de la capa de mezcla es fija durante todo el periodo seleccionado, la estabilidad térmica de la columna de agua cambia a lo largo del año. En los ecosistemas templados, existen por lo general dos periodos de mezclas (lagos dimícticos), uno en otoño después de una estratificación durante el verano y otro en primavera después de la estratificación del invierno que se produce cuando la temperatura de la superficie cae por debajo de 4 \\(^oC\\) y lás aguas más cálidas quedan sumergidas. Aquí en nuestro clima meditarráneo, tenemos por lo general un solo perido de mezcla (lagos monomícticos), en otoño. Esto es debido a que las temperaturas del invierno no son tan bajas como para generar una termoclina inversa. Si habéis tenido suerte en la búsqueda de los datos que os pedí (recordad que buscabamos información sobre temperatura a distintas profundidas durante el periodo que seleccionamos para el lago Trout), podemos intentar cálcular a que profundidad se encuentra el metalimnion y, por ende, cual es la profundidad de la capa de mezcla (z.mix). Si no los encontrastéis no os preocupéis, aquí podéis decargarlos. La resolución que nos interesa es horaria (igual que la de los demás datos con los que hemos trabajado). Aquí os dejo el código para descargar directamente los datos que necesitamos a un objeto llamado dt2: inUrl2 &lt;- &quot;https://pasta.lternet.edu/package/data/eml/knb-lter-ntl/116/26/c4aeea97f85c26cc39781d6b8dd40cb3&quot; infile2 &lt;- tempfile() download.file(inUrl2,infile2,method=&quot;curl&quot;) dt2 &lt;-read.csv(infile2,header=F ,skip=1 ,sep=&quot;,&quot; ,quot=&#39;&quot;&#39; , col.names=c( &quot;sampledate&quot;, &quot;year4&quot;, &quot;month&quot;, &quot;daynum&quot;, &quot;hour&quot;, &quot;depth&quot;, &quot;wtemp&quot;, &quot;flag_wtemp&quot; ), check.names=TRUE) # Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings # attempting to convert dt2$sampledate dateTime string to R date structure (date or POSIXct) tmpDateFormat&lt;-&quot;%Y-%m-%d&quot; tmp2sampledate&lt;-as.Date(dt2$sampledate,format=tmpDateFormat) # Keep the new dates only if they all converted correctly if(length(tmp2sampledate) == length(tmp2sampledate[!is.na(tmp2sampledate)])){dt2$sampledate &lt;- tmp2sampledate } else {print(&quot;Date conversion failed for dt2$sampledate. Please inspect the data and do the date conversion yourself.&quot;)} rm(tmpDateFormat,tmp2sampledate) if (class(dt2$year4)==&quot;factor&quot;) dt2$year4 &lt;-as.numeric(levels(dt2$year4))[as.integer(dt2$year4) ] if (class(dt2$year4)==&quot;character&quot;) dt2$year4 &lt;-as.numeric(dt2$year4) if (class(dt2$month)==&quot;factor&quot;) dt2$month &lt;-as.numeric(levels(dt2$month))[as.integer(dt2$month) ] if (class(dt2$month)==&quot;character&quot;) dt2$month &lt;-as.numeric(dt2$month) if (class(dt2$daynum)==&quot;factor&quot;) dt2$daynum &lt;-as.numeric(levels(dt2$daynum))[as.integer(dt2$daynum) ] if (class(dt2$daynum)==&quot;character&quot;) dt2$daynum &lt;-as.numeric(dt2$daynum) if (class(dt2$depth)==&quot;factor&quot;) dt2$depth &lt;-as.numeric(levels(dt2$depth))[as.integer(dt2$depth) ] if (class(dt2$depth)==&quot;character&quot;) dt2$depth &lt;-as.numeric(dt2$depth) if (class(dt2$wtemp)==&quot;factor&quot;) dt2$wtemp &lt;-as.numeric(levels(dt2$wtemp))[as.integer(dt2$wtemp) ] if (class(dt2$wtemp)==&quot;character&quot;) dt2$wtemp &lt;-as.numeric(dt2$wtemp) if (class(dt2$flag_wtemp)!=&quot;factor&quot;) dt2$flag_wtemp&lt;- as.factor(dt2$flag_wtemp) # Convert Missing Values to NA for non-dates # Here is the structure of the input data frame: str(dt2) attach(dt2) # The analyses below are basic descriptions of the variables. After testing, they should be replaced. summary(sampledate) summary(year4) summary(month) summary(daynum) summary(hour) summary(depth) summary(wtemp) summary(flag_wtemp) detach(dt2) Una vez hemos descargado los datos vamos a seleccionar el periodo que nos interesa, en función a los datos que ya teníamos seleccionado: #Cargamos los datos que ya teníamos seleccionados datos.ts &lt;- read.csv(&quot;./Datos/datos_ts_2005.csv&quot;) datos.ts$datetime &lt;- as.POSIXct(datos.ts$datetime) #Vamos a darle un formato a la fecha homogéneo y coherente con los demás objetos con los que hemos trabajado. dt2$datetime &lt;- as.POSIXct(paste(dt2$sampledate, dt2$hour/100), format = &quot;%Y-%m-%d %H&quot;) #Seleccionamos el mismo periodo Perfiles_temp &lt;- subset(dt2, datetime &gt;= min(datos.ts$datetime) &amp; datetime &lt;= max(datos.ts$datetime)) rm(dt2) #Eliminamos el archivo para descargar la memoria Vamos a guardar los datos para más adelante. write.csv(Perfiles_temp, &quot;./Datos/Perfiles_temp.csv&quot;, row.names = FALSE) Una vez seleccionado los datos, podemos empezar a pensar en como calcular la profundidad de la capa de mezcla. El paquete con el que estamos trabajando (LakeMetabolizer) cuenta con una función para ello. Esta función, en realidad, es importada de otro paquete muy interesante, rLakeAnalyzer, para los que tengan pensado trabajar con datos limnológicos. En esta práctica no tendremos tiempo de verlo pero merece al menos ser nombrado. La función de la que hablamos es ts.meta.depths(). Esta función nos dice entre que profundidades se encuentra el metalimnion. Por encima de este tendremos el epiliminion (capa de mezcla) y por debajo el hipolimion. Si leemos la información de la función (?ts.meta.depths), nos explica los datos que tenemos que proporcionarle y cómo. Antes de nada, tenemos que modificar el formato de nuestra tabla de datos, vamos a ello: library(reshape2) #Transformamos los datos al formato &#39;ancho&#39; dt_wtemp &lt;- dcast(Perfiles_temp, datetime ~ depth, value.var = &quot;wtemp&quot;) head(dt_wtemp) ## datetime 0 1 2 3 4 5 6 7 ## 1 2005-09-01 00:00:00 20.401 20.280 20.407 20.497 20.279 20.478 20.222 20.291 ## 2 2005-09-01 01:00:00 20.333 20.218 20.349 20.424 20.205 20.421 20.225 20.390 ## 3 2005-09-01 02:00:00 20.254 20.148 20.274 20.345 20.133 20.341 20.141 20.350 ## 4 2005-09-01 03:00:00 20.193 20.088 20.213 20.302 20.085 20.296 20.120 20.318 ## 5 2005-09-01 04:00:00 20.172 20.058 20.189 20.247 20.044 20.258 20.092 20.290 ## 6 2005-09-01 05:00:00 20.133 20.013 20.145 20.230 20.002 20.217 20.063 20.241 ## 7.5 8 8.5 9 9.5 10 10.5 11 11.5 12 12.5 ## 1 20.011 20.102 20.075 19.976 19.960 19.432 16.357 12.677 11.205 10.580 10.075 ## 2 20.137 20.208 20.107 19.957 19.952 19.272 16.155 13.120 11.506 NA 10.241 ## 3 20.130 20.274 20.235 20.059 20.005 19.005 15.901 12.507 11.441 NA 10.276 ## 4 20.116 20.284 20.295 20.174 20.130 19.516 16.128 13.165 11.619 10.613 10.014 ## 5 20.093 20.253 20.289 20.157 20.084 19.620 16.373 13.092 11.438 10.666 10.093 ## 6 20.041 20.211 20.235 20.123 20.088 18.893 16.207 13.479 11.761 10.891 10.275 ## 13 13.5 14 16 19 23 27 31 ## 1 9.453 9.389 9.024 8.181 7.717 7.129 6.983 6.466 ## 2 9.518 9.338 8.972 8.113 7.696 7.129 6.974 6.447 ## 3 9.611 9.463 9.094 8.130 7.661 7.090 6.953 6.427 ## 4 9.378 9.280 8.910 8.031 7.682 7.157 6.956 NA ## 5 9.569 9.418 9.013 8.196 7.683 7.171 6.923 NA ## 6 9.535 9.381 8.976 8.054 7.534 7.111 6.948 6.452 #Renombramos las columnas según nos pide la función colnames(dt_wtemp)[-1] &lt;- paste(&quot;wtr&quot;, colnames(dt_wtemp)[-1], sep = &#39;_&#39;) head(dt_wtemp) ## datetime wtr_0 wtr_1 wtr_2 wtr_3 wtr_4 wtr_5 wtr_6 wtr_7 ## 1 2005-09-01 00:00:00 20.401 20.280 20.407 20.497 20.279 20.478 20.222 20.291 ## 2 2005-09-01 01:00:00 20.333 20.218 20.349 20.424 20.205 20.421 20.225 20.390 ## 3 2005-09-01 02:00:00 20.254 20.148 20.274 20.345 20.133 20.341 20.141 20.350 ## 4 2005-09-01 03:00:00 20.193 20.088 20.213 20.302 20.085 20.296 20.120 20.318 ## 5 2005-09-01 04:00:00 20.172 20.058 20.189 20.247 20.044 20.258 20.092 20.290 ## 6 2005-09-01 05:00:00 20.133 20.013 20.145 20.230 20.002 20.217 20.063 20.241 ## wtr_7.5 wtr_8 wtr_8.5 wtr_9 wtr_9.5 wtr_10 wtr_10.5 wtr_11 wtr_11.5 wtr_12 ## 1 20.011 20.102 20.075 19.976 19.960 19.432 16.357 12.677 11.205 10.580 ## 2 20.137 20.208 20.107 19.957 19.952 19.272 16.155 13.120 11.506 NA ## 3 20.130 20.274 20.235 20.059 20.005 19.005 15.901 12.507 11.441 NA ## 4 20.116 20.284 20.295 20.174 20.130 19.516 16.128 13.165 11.619 10.613 ## 5 20.093 20.253 20.289 20.157 20.084 19.620 16.373 13.092 11.438 10.666 ## 6 20.041 20.211 20.235 20.123 20.088 18.893 16.207 13.479 11.761 10.891 ## wtr_12.5 wtr_13 wtr_13.5 wtr_14 wtr_16 wtr_19 wtr_23 wtr_27 wtr_31 ## 1 10.075 9.453 9.389 9.024 8.181 7.717 7.129 6.983 6.466 ## 2 10.241 9.518 9.338 8.972 8.113 7.696 7.129 6.974 6.447 ## 3 10.276 9.611 9.463 9.094 8.130 7.661 7.090 6.953 6.427 ## 4 10.014 9.378 9.280 8.910 8.031 7.682 7.157 6.956 NA ## 5 10.093 9.569 9.418 9.013 8.196 7.683 7.171 6.923 NA ## 6 10.275 9.535 9.381 8.976 8.054 7.534 7.111 6.948 6.452 Ahora sí podemos aplicar la función: library(LakeMetabolizer) metalimnion &lt;- ts.meta.depths(dt_wtemp, na.rm = TRUE) La función tiene el argumento slope para que le indiquemos a partir de que gradiente de densidad consideramos que existe termoclina, el defecto es 0.1 y nosotros lo vamos a dejar así. Este es el resultado: head(metalimnion) ## datetime top bottom ## 1 2005-09-01 00:00:00 9.474704 12.14086 ## 2 2005-09-01 01:00:00 9.429445 12.87880 ## 3 2005-09-01 02:00:00 9.352283 12.21288 ## 4 2005-09-01 03:00:00 9.425544 12.78590 ## 5 2005-09-01 04:00:00 9.468246 12.44277 ## 6 2005-09-01 05:00:00 9.341803 12.44117 Ahora seleccionamos la parte superior del metalimnion como límite de nuestra capa de mezcla. #Reemplazamos los datos de la variable que ya habiamos creado de z.mix datos.ts$z.mix &lt;- metalimnion$top #Guardamos los datos para más adelante write.csv(datos.ts, &quot;./Datos/datos_ts_2005_zmix.csv&quot;, row.names = FALSE) Pues bien, ahora que ya hemos cálculado la profundidad de la capa de mezcla y la hemos añadido a nuestra tabla de datos, podemos volver a calcular las tasa metabólicas, como ya hemos hecho anteriormente. Esto os lo dejo a vosotros… "],["representación-gráfica.html", "7 Representación gráfica 7.1 Variables físico-químicas 7.2 Estructura térmica del lago. 7.3 Resultados del metabolismo", " 7 Representación gráfica Después de algunas peleas con los datos y, porqué no ser sinceros, con el maldito-bendito R, ya tenemos suficiente información sobre el lago Trout. El siguiente paso es representar gráficamente esta información para poder interpretar mejor los resultados y comunicarlos de forma efectiva al resto de compañerxs. El paquete estrella de R para esta tarea, ya lo conocéis, es ggplot2. Antes de empezar vamos a cargar todos los datos que necesitaremos. #Datos iniciales datos.ts &lt;- read.csv(&quot;./Datos/datos_ts_2005_zmix.csv&quot;) datos.ts$datetime &lt;- as.POSIXct(datos.ts$datetime) #Datos temperatura Perfiles_temp &lt;- read.csv(&quot;./Datos/Perfiles_temp.csv&quot;) Perfiles_temp$sampledate &lt;- as.Date(Perfiles_temp$sampledate) Perfiles_temp$datetime&lt;- as.POSIXct(Perfiles_temp$datetime) #Datos metabolismo datos_metab &lt;- read.csv(&quot;./Datos/Metabolismo_Trout.csv&quot;) 7.1 Variables físico-químicas Lo primero que vamos a hacer es generar algunas sencillas gráficas para ver el estado del lago durante el periodo que hemos seleccionado. library(ggplot2) ggplot(datos.ts, aes(x=datetime, y=do.obs))+ geom_point() Podemos repetir esta gráfica para cada una de las variables que tenemos o podemos servirnos del paquete reshape2 y convertir la tabla de datos a forma “largo” y usar una capa para que haga una gráfica por cada variable. library(reshape2) #Convertimos la tabla a formato largo datos.ts_l &lt;- melt(datos.ts, id.vars = c(&quot;datetime&quot;) , varnames = c(&#39;variable&#39;), value.name = &#39;valor&#39;) #Representamos ggplot(datos.ts_l, aes(x = datetime, y = valor))+ geom_point()+ facet_wrap(~variable, scales = &quot;free_y&quot;, ncol = 2) También podríamos, por ejemplo, querer hacer un “zoom” a solo unos días para ver si existen tendencencias diarias. library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union ggplot(subset(datos.ts_l, date(datetime) &gt;= &quot;2005-09-05&quot; &amp; date(datetime) &lt;= &quot;2005-09-07&quot;), aes(x = datetime, y = valor))+ geom_point()+ facet_wrap(~variable, scales = &quot;free_y&quot;, ncol = 2)+ scale_x_datetime(date_labels = &quot;%R&quot;) Vamos a modificar un poco el aspecto de las gráficas anteriores antes de pasar las siguientes: #Podemos usar un &quot;theme&quot; prediseñado Grafica_vars &lt;- ggplot(datos.ts_l, aes(x = datetime, y = valor))+ geom_point()+ facet_wrap(~variable, scales = &quot;free_y&quot;, ncol = 2) Grafica_vars + theme_classic(base_size = 12, base_family = &quot;Times&quot;) Grafica_vars + labs(x = NULL, y = NULL) #Y luego cambiar algunos aspecto del &quot;theme&quot; de la gráfica usando: %+replace% theme(axis.title = element_blank(), strip.background = element_blank()) Grafica_vars + theme_classic(base_size = 12, base_family = &quot;Times&quot;)%+replace% theme(axis.title = element_blank(), strip.background = element_blank()) #También podemos modificar el &quot;theme&quot; directamente a nuestro gusto Grafica_vars + theme(axis.title = element_blank(), panel.grid = element_blank(), plot.background = element_rect(fill = &quot;black&quot;)) Una vez que tenemos el aspecto que deseamos de nuestra gráfica, la guardamos y la exportamos, a la carpeta que creamos llamada “Graficas”, para poder usarla en cualquier documento en el que estemos trabajando. #Guardamos Grafica_vars_exp &lt;- Grafica_vars + theme_classic(base_size = 12, base_family = &quot;Times&quot;) + labs(x = NULL, y = NULL) #Exportamos ggsave(&quot;./Graficas/Grafica_variables.png&quot;, Grafica_vars_exp, width = 15, height = 20, units = &quot;cm&quot;) 7.2 Estructura térmica del lago. Como finalmente conseguimos encontrar datos de la temperatura del lago a distintas profundidades sería interesante realizar una gráfica de contorno del periodo estudiado. Estas gráficas son muy visuales y ayudan a entender mejor el comportamiento del lago. Las vemos a menudo en los artículos. Imagen ejemplo de gráfica de contorno extraída de Vidal et al. (2010) Existen distintos programas que permiten hacer estas gráficas como son Surfer, SigmaPlot o OceanDataView, con el inconveniente que la mayoría de ellos son privativos y de pago (OceanDataView se salva!). Además, ya que estamos metidos en harina porque no hacerlo todo con el mismo programa. R nos permite trabajar desde los datos crudos hasta la generación de gráficas e informes (estos documentos están hechos directamente desde R). Vamos a partir de la tabla de datos en formato “ancho” que creamos para calcular la profundidad de la capa de mezcla. library(reshape2) #Así creamos el archivo dt_wtemp &lt;- dcast(Perfiles_temp, datetime ~ depth, value.var = &quot;wtemp&quot;) #Renombramos las columnas según nos pide la función colnames(dt_wtemp)[-1] &lt;- paste(&quot;wtr&quot;, colnames(dt_wtemp)[-1], sep = &#39;_&#39;) Al transformar el objeto a formato “ancho” hemos homogeneizado las profundidades a las que tenemos datos de temperatura. Es decir, tenemos el mismo número de medidas y a las mismas profundidades para cada momento. Sin embargo, como los datos no estaban completos se han introducidos NAs en aquellas profundidades donde no teníamos información. Por ejemplo, en el perfil de las 1:00 am del 205-09-01, tenemos datos de temperatura a 11.5 y a 12.5 metros pero no a 12 m. Para solucionar esto, podemos interpolar los datos. library(zoo) ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric #Interpolamos dt_wtemp[,-1] &lt;- na.approx(dt_wtemp[,-1],rule = 2) Ahora podemos devolver los datos a su formato largo. library(readr) Per_melt &lt;- melt(dt_wtemp, id.vars = c(&quot;datetime&quot;)) Per_melt$variable &lt;- parse_number(as.character(Per_melt$variable)) colnames(Per_melt) &lt;- c(&quot;datetime&quot;, &quot;depth&quot;, &quot;wtemp&quot;) E intentamos representar. ggplot(Per_melt, aes(x = datetime, y = depth, color = wtemp))+ geom_point() Parece que estamos bastante cerca, sin embargo, hay muchos huecos en blanco. Para solucionar esto vamos a crear una matriz y a usar una interpolación espacial multinivel b-spline para completar la información que nos falta. library(lubridate) #Primero tenemos que convertir las fechas en un vector numérico Per_melt$datetime &lt;- decimal_date(Per_melt$datetime) library(MBA) # Aquí creamos una matriz con mayor resolución usando una interpolación espacial multinivel b-spline Temp_mba &lt;- mba.surf(Per_melt, no.X = 500, no.Y = 500, extend = T) #Aquí están las fecha con la nueva resolución (en este caso es menor de la que teniamos, 500 &quot;perfiles&quot;, frente a los 1464 que teníamos, pero no necesitamos más) head(Temp_mba$xyz.est$x) ## [1] 2005.666 2005.666 2005.666 2005.667 2005.667 2005.667 #Aquí tenemos las nuevas profundidades, 500 profundidades en lugar de las 27 que teniamos antes head(Temp_mba$xyz.est$y) ## [1] 0.00000000 0.06212425 0.12424850 0.18637275 0.24849699 0.31062124 #Estos son los datos de temperatura head(Temp_mba$xyz.est$z)[1:10] ## [1] 20.25121 20.22306 20.17375 20.10752 20.02856 19.94159 20.23992 20.21190 ## [9] 20.16351 20.09884 #Los juntamos todos dimnames(Temp_mba$xyz.est$z) &lt;- list(Temp_mba$xyz.est$x, Temp_mba$xyz.est$y) #Y los volvemos al formato largo. library(dplyr) Temp_mba &lt;- melt(Temp_mba$xyz.est$z, varnames = c(&#39;date&#39;, &#39;depth&#39;), value.name = &#39;temp&#39;) %&gt;% mutate(temp = round(temp, 3)) #Esta es la pinta de los datos: head(Temp_mba) ## date depth temp ## 1 2005.666 0 20.251 ## 2 2005.666 0 20.223 ## 3 2005.666 0 20.174 ## 4 2005.667 0 20.108 ## 5 2005.667 0 20.029 ## 6 2005.667 0 19.942 Ahora representamos de nuevo. ggplot(Temp_mba, aes(x = date, y = depth, color = temp))+ geom_point() ¡Mucho mejor! Vamos a cambiar algunos aspectos estéticos para que quede más resultona. #Vamos a devolverle el formato de fecha Temp_mba$date &lt;- date_decimal(Temp_mba$date) #Cargamos un paquete para usar una paleta de color má común library(colorRamps) Grafica_temp &lt;- ggplot(data = Temp_mba, aes(x = date, y = depth)) + geom_tile(aes(fill = temp)) + #Usamos esta capa que viene mejor para este tipo de gráficos pero podíamos haber usado geom_point scale_y_reverse()+ scale_fill_gradientn(colours = matlab.like2(10)) + geom_contour(aes(z = temp), binwidth = 2, colour = &quot;black&quot;, alpha = 0.2) + labs(y = &quot;Profundidad (m)&quot;, x = NULL, fill = &quot;temp. (°C)&quot;) + coord_cartesian(expand = 0) Grafica_temp Hemos cambiado el formato de la fecha, hemos invertido el eje profundidad para que se más intuitivo, hemos cambiado las etiquetas, el color y añadido unas lineas de contorno. Vamos a probar a añadirle la profundidad de la capa de mezcla. Grafica_temp_zmix &lt;- Grafica_temp + geom_line(data = datos.ts, aes(x=datetime, y = z.mix, color = &quot;Capa mezcla&quot;), size = 0.2)+ scale_color_manual(values = &quot;black&quot;) + labs(color = NULL) Grafica_temp_zmix Y ya la tenemos lista para exportar. ggsave(&quot;./Graficas/Grafica_temp.png&quot;, Grafica_temp_zmix, width = 20, height = 10, units = &quot;cm&quot;) 7.3 Resultados del metabolismo Por último, vamos a representar los resultados que hemos obtenido del metabolismo del lago Trout. Como hemos hecho anteriormente convertimos los datos a formato “largo” y representamos. #transformamos datos_metab_long &lt;- melt(datos_metab, id.vars = c(&quot;year&quot;, &quot;doy&quot;)) #Vamos a crear una variable fecha que convierta el día del año (doy) en dia mes y año datos_metab_long$date &lt;- as.Date(datos_metab_long$doy, origin = &quot;2004-12-31&quot;) #representamos Grafica_metab &lt;- ggplot(datos_metab_long, aes(x = date, y = value, color = variable))+ geom_point() Grafica_metab #vamos a modificar un poco los aspectos estéticos Grafica_metab &lt;- Grafica_metab + scale_color_manual(values = c(&quot;Green&quot;, &quot;Brown&quot;, &quot;Darkgreen&quot;))+ labs(x= NULL, y = expression(paste(&quot;mg O&quot;[2], &quot;·L&quot;^{-1},&quot;día&quot;^{-1}, sep = &quot;&quot;)), color = NULL )+ theme_classic(base_size = 12, base_family = &quot;Times&quot;) Grafica_metab #La exportamos ggsave(&quot;./Graficas/Metabolismo_Trout_sept-oct.png&quot;, Grafica_metab, width = 15, height = 10, units = &quot;cm&quot;) Si queremos obtener una visión global del metabolismo durante todo el periodo estudiado podemos probar con un tipo de gráfica de cajas y bigotes. Grafica_metab_boxplot &lt;- ggplot(datos_metab_long, aes(x = variable, y = value))+ geom_boxplot() Grafica_metab_boxplot #Vamos a mejorarla un poco estéticamente. Grafica_metab_boxplot &lt;- ggplot(datos_metab_long, aes(x = variable, y = value, fill = variable))+ geom_boxplot()+ scale_fill_manual(values = c(&quot;Green&quot;, &quot;Brown&quot;, &quot;Darkgreen&quot;))+ labs(x= NULL, y = expression(paste(&quot;mg O&quot;[2], &quot;·L&quot;^{-1},&quot;día&quot;^{-1}, sep = &quot;&quot;)), fill = NULL )+ theme_classic(base_size = 12, base_family = &quot;Times&quot;) Grafica_metab_boxplot Esta es un representación más transparente de nuestros datos, en la que podemos ver la mediana (la linea), los cuartiles 1 y 4 (los límites de la caja) y los valores que se salen del rango. Sin embargo, sé que nos han acostumbrado a representar la media y la desviación estándar. Como os puede ser de utilidad en el futuro (no tan lejano, para el TFM por ejemplo) vamos a ver como hacerlo. #Primero debemos calcular la media y la desviación estándar library(plyr) ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize Metab_mSd &lt;- ddply(datos_metab_long, ~variable, summarize, Media = mean(value), Sd = sd(value)) #Aquí lo tenemos Metab_mSd ## variable Media Sd ## 1 GPP 0.30275860 0.3286279 ## 2 R -0.24464004 0.2912254 ## 3 NEP -0.05776527 0.1970150 #Y luego lo representamos ggplot(Metab_mSd, aes(x = variable, y = Media))+ geom_point()+ geom_errorbar(aes(ymin = Media-Sd, ymax = Media+Sd)) #Vamos a cambiar algunos aspectos estéticos Grafica_metab_mSd &lt;- ggplot(Metab_mSd, aes(x = variable, y = Media))+ geom_point(size = 4)+ geom_errorbar(aes(ymin = Media-Sd, ymax = Media+Sd), width = 0.2)+ labs(x= NULL, y = expression(paste(&quot;mg O&quot;[2], &quot;·L&quot;^{-1},&quot;día&quot;^{-1}, sep = &quot;&quot;)))+ theme_classic(base_size = 12, base_family = &quot;Times&quot;) Grafica_metab_mSd #Y exportamos ggsave(&quot;./Graficas/Grafica_metab_mSd.png&quot;, Grafica_metab_mSd, width = 10, height = 10, units = &quot;cm&quot;) Una vez ya tenéis los datos más importantes organizados y representados y las estimas del metabolismo calculadas tocaría, con los conocimientos teóricos de los que disponéis, interpretar esta información. Referencias Vidal, J., E. Moreno-Ostos, C. Escot, R. Quesada, and F. Rueda. 2010. The effects of diel changes in circulation and mixing on the longitudinal distribution of phytoplankton in a canyon-shaped Mediterranean reservoir. Freshwater Biology 55: 1945–1957. "],["recopilamos.html", "8 Recopilamos", " 8 Recopilamos Una vez llegados a este punto hemos: Conocido una red de trabajo internacional colaborativa que nos ofrece datos de gran interés sobre sistemas acuáticos, así como buscadores para acceder a este tipo de información. Descargado esta información y trabajado con gran cantidad de datos, que con las hojas de cálculo tradicionales (Calc o Excel) hubiera sido, siendo politicamente correctos, bastante tedioso. Conocido algunas de la funciones que nos ofrece el paquete LakeMetabolizer para calcular o estimar distintas variables de gran utilidad e interés en limnología. Representado la información obtenida con la calidad necesaria para su publicación en revistas científicas. "],["referencias.html", "Referencias", " Referencias Cole, J. J., and N. F. Caraco. 1998. Atmospheric exchange of carbon dioxide in a low-wind oligotrophic lake measured by the addition of SF 6. Limnology and Oceanography 43: 647–656. Dugan, H. A., R. Iestyn Woolway, A. B. Santoso, J. R. Corman, A. Jaimes, E. R. Nodine, V. P. Patil, J. A. Zwart, J. A. Brentrup, A. L. Hetherington, S. K. Oliver, J. S. Read, K. M. Winters, P. C. Hanson, E. K. Read, L. A. Winslow, and K. C. Weathers. 2016. Consequences of gas flux model choice on the interpretation of metabolic balance across 15 lakes. Inland Waters 6: 581–592. Hanson, P. C., S. R. Carpenter, N. Kimura, C. Wu, S. P. Cornelius, and T. K. Kratz. 2008. Evaluation of metabolism models for free-water dissolved oxygen methods in lakes. Limnology and Oceanography: Methods 6: 454–465. Mccutchan, J. H., W. M. Lewis, and J. F. Saunders. 1998. Uncertainty in the estimation of stream metabolism from open-channel oxygen concentrations. Journal of the North American Benthological Society 17: 155–164. Odum, H. T. 1956. Primary Production in Flowing Waters. Limnology and Oceanography 102–117. Staehr, P. a., D. Bade, G. R. Koch, C. Williamson, P. Hanson, J. J. Cole, and T. Kratz. 2010. Lake metabolism and the diel oxygen technique: State of the science. Limnology and Oceanography: Methods 8: 628–644. Staehr, P. A., and K. Sand-Jensen. 2007. Temporal dynamics and regulation of lake metabolism. Limnology and Oceanography 52: 108–120. Vidal, J., E. Moreno-Ostos, C. Escot, R. Quesada, and F. Rueda. 2010. The effects of diel changes in circulation and mixing on the longitudinal distribution of phytoplankton in a canyon-shaped Mediterranean reservoir. Freshwater Biology 55: 1945–1957. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
